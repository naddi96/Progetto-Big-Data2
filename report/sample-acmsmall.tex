%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
 \documentclass[acmsmall]{acmart}
\usepackage{listings}
\lstset{
	language=bash,
	basicstyle=\ttfamily
}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
%%\AtBeginDocument{%
 %% \providecommand\BibTeX{{%
  %%  \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.



%%
%% These commands are for a JOURNAL article.
%% \acmJournal{JACM}
%% \acmVolume{37}
%% \acmNumber{4}
%% \acmArticle{111}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Progetto 2 [SABD]}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Damiano Nardi}

\email{damiano6276@gmail.com}
\affiliation{%
 \institution{TorVergata, Corso Di Informatica}
 }

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.

%%
%% The abstract is a short summary of the work to be presented in the

%\begin{abstract}

%\end{abstract}
%% article.

%%
%% The code below is generated by the tool at %%http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.



%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.dsd
 \maketitle{} 

\section{Introduzione}
In questa relazione si descriverà il lavoro svolto che consiste nella realizzazione delle query(1,2) 


\section{Processamento Delle Query}
Per processare le query 1 e 2 è stato usato il framework apache flink usando le api java, inoltre per confrontare flink con kafka streams è stata fatta la anche query 1 usando kafka streams.
Si è utilizzata una macchina virtuale con Ubuntu 20.04 con allocatevi 4 core della cpu (i7-8750H) e 8gb di ram (ddr4 2400 MHz).

\subsection{Stremming del dataset}
Il dataset viene letto e inviato riga per riga a una topica kafka che viene poi consumata da apache flink o da kafka streams.

\subsection{Query1}
\begin{quote}
query 1
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa.\end{quote}

Lo stream del dataset viene letto da una topica kafka la prima operazione fatta 
è una 
\subsubsection{flatMap} 
In questa flatMap viene processata la singola ringa del dataset andando a mettere come chiave il campo Boro (il quartiere) e come valore un ogetto con questi campi: OccurredOn (data del ritardo), HowLongelayed (ritardo in minuti ) e altri campi meno importanti;
le righe del dataset che non hanno il campo HowLongDelayed o lo hanno in un formato non previsto vengono ignorate.
\subsubsection{EventTime e TumblingEventTimeWindows}
A questo punto flink estrare l'event time dai dati processati della flatMap e 
subito dopo crea una TumblingWindows della durata preimpostata tra 1,7,30 giorni

\subsubsection{Reduce}
Facciamo una reduce (abbiamo che chiave Boro), essenzialmente andiamo a sommare i ritardi di un quartiere (nella stessa finestra) 
teniamo anche in considerazione la "somma" delle reduce fatte e la data più vecchia in ogni reduce.

\subsubsection{Map}
In questa Map andiamo a calcolare la media dei ritardi sommati per ogni quartiere nella precedente reduce

\subsubsection{Apply}
In fine vengono raggruppati tutti i dati nella stessa finestra temporale e viene formattato l'output come richiesto dalle specifiche.
\subsubsection{addSink}
Infine viene aggiunto un sink che manda lo stream processato a una topica kafka

\subsection{Query 1 kafka streams}
La query 1 è stata fatta anche su kafka streams le operazioni fatte sono più o meno le stesse, cambia la sintassi, quindi per evitare ridondanza non verranno riportate.


\subsection{Query2}
\begin{quote}
query2 .\end{quote}

Lo stream del dataset viene letto da una topica kafka la prima operazione fatta 
è una 

\subsubsection{flatMap}
In questa flatMap viene processata la singola riga del dataset andando a mettere come chiave il campo Reason con il prefisso "fascia5-11" o "fascia12-19" scelto in base al campo OccuredOn mentre come valore un ogetto con i seguenti attributi
OccurredOn,Reason,rank(intero inizialmente pari a 1 ), list(inizialmente vuota).


\subsubsection{EventTime e TumblingEventTimeWindows}
A questo punto flink estrare l'event time dai dati processati della flatMap e 
subito dopo crea una TumblingWindows della durata preimpostata tra 1 o 7 giorni


\subsubsection{Reduce}
In questa reduce vengono sommati i campi rank e viene presa la data (OccurredOn) minima tra le varie operazioni di reduce 


\subsubsection{Map}
Viene messa la tupla (Reason,rank) dentro la lista e mettiamo come chiave "fascia5-11" o "fascia12-19"

\subsubsection{TumblingEventTimeWindows}
specifichiamo ancora la finestra temporale

\subsubsection{Reduce}
In questa reduce viene fatto il l'unione del campo lista (che avevamo iniziato a popolare nella map precedente) per le due fascie "fascia5-11", "fascia12-19"

\subsubsection{Map}
Adesso prendiamo la dal campo list le 3 Reason che hanno il rank più alto

\subsubsection{TumblingEventTimeWindows}
Specifichiamo ancora la finestra temporale

\subsubsection{Reduce e Map}
In fine facciamo una Reduce e una Map ai fine formattare l'output come richiesto dalle specifiche
\subsubsection{addSink}
Viene aggiunto un sink che manda lo stream processato a una topica kafka


\section{Tempi}

Sono stati misurati i tempi di latenza e throughput di messaggi al secondo,
In questo paragrafo descriveremo come sono stati calcolati 


\subsection{Producer}
Come accennato prima abbiamo un producer che legge il dataset riga per riga  e li passa a una topica kafka durante questa operazione appena prima di inserire la riga sulla topica viene aggiunto a inizio riga il timestamp corrente.


\subsection{2 tipo di latenza}
Quando viene fatta una reduce andiamo ad unificare più righe del dataset che sono state inserite nella topica a tempi di processamento diversi, durante queste operazioni viene tenuta traccia del tempo di processamento più vecchio e più nuovo, quindi ogni output prodotto da flink e da kafka conterrà 3 tempi:
event time,processing time riga più vecchia,processing time riga più nuova.
Abbiamo la latenza del record più vecchio e la latenza del record più nuovo
per ogni output da flink/kafka

\subsection{Consumer}
Il consumer non fa altro che leggere da una topica kafka (output di flink o kafka streams) registra il timestamp corrente ad ogni record prelevato da quest'ultima e calcola i due tempi di latenza in secondi andando a fare la differnza tra quello corrente e quelli presnti nell'output,

\subsection{throughput}
Per misurare il throughput ho utilizzato questo comando di kafka streams
\begin{lstlisting}[language=bash]
/home/naddi/kafka/bin/kafka-consumer-perf-test.sh 
--topic output-stream
 --broker-list "localhost:9092,localhost:9093,localhost:9094" 
 --messages 900 --threads 1
\end{lstlisting}
che tra le misure che ritorna vi è anche la latenza











\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
